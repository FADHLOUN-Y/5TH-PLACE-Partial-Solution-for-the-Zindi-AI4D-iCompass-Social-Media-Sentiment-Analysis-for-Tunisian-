general:
  base_dir: ./
  logs_dir: lightning_logs/
  seed: 1
  num_workers: 2
  gpu_list:
  - 0
model:
  architecture_name: bert_lstm
  sequence_max_length: 64
  lstm_hidden_size: 128
training:
  train_csv: data/Train.csv
  batch_size: 32
  lr: 3.0e-05
  max_epochs: 100
  min_epochs: 3
  val_metric: val_accuracy
  metric_mode: max
testing:
  test_csv: data/Test.csv
callbacks:
  tensorboard:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${general.logs_dir}
    name: ${model.architecture_name}_random_${general.seed}_${classifiermode.num_classes}Labels
    version: seed_${general.seed}
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: ${training.val_metric}
    mode: ${training.metric_mode}
    patience: 2
    verbose: true
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${general.logs_dir}/checkpoints/${model.architecture_name}${classifiermode.num_classes}/
    filename: random_${general.seed}
    monitor: ${training.val_metric}
    mode: ${training.metric_mode}
    period: 1
    save_top_k: 1
    save_last: false
    verbose: true
    save_weights_only: true
scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
optimizer:
  _target_: transformers.AdamW
  lr: ${training.lr}
classifiermode:
  num_classes: 1
  valid_positive_text_samples: 0.5
  valid_negative_text_samples: 0.5
  valid_neutral_text_samples: 0
  threshhold: 0.5
  valid_samples: 500
loss_fn:
  _target_: torch.nn.BCEWithLogitsLoss
models:
  _target_: transformers.BertModel.from_pretrained
  pretrained_model_name_or_path: bert-base-multilingual-cased
tokenizer:
  _target_: transformers.BertTokenizer.from_pretrained
  pretrained_model_name_or_path: bert-base-multilingual-cased
  do_lower_case: false
